{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1vCReuAKT7cf0NYti7ixkDE5EtdMy2hbN","authorship_tag":"ABX9TyPkl/ZZIp8kTR2EAWw00rSO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Su45XioLjLjd","executionInfo":{"status":"ok","timestamp":1711352413443,"user_tz":-480,"elapsed":612,"user":{"displayName":"chris peng","userId":"18113039730144201133"}},"outputId":"a0bfbc6e-2089-49bf-a6fa-deb3d6a978ad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import mmap\n","import random\n","import pickle\n","import argparse\n","import pickle\n","parser = argparse.ArgumentParser(description='This is a demonstration program')\n","\n","import os\n","import re"],"metadata":{"id":"qjJSQvLmkPOi","executionInfo":{"status":"ok","timestamp":1712470332703,"user_tz":-480,"elapsed":4609,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["checkpoint_path='/content/drive/MyDrive/checkpoints/'"],"metadata":{"id":"xhjx721oOtUm","executionInfo":{"status":"ok","timestamp":1712470372665,"user_tz":-480,"elapsed":620,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# prompt: mount google drive\n","\n","#from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"Q8lqIGIUdYOL","executionInfo":{"status":"error","timestamp":1712470353764,"user_tz":-480,"elapsed":982,"user":{"displayName":"chris peng","userId":"18113039730144201133"}},"outputId":"cbda8d31-7858-457e-ca0b-57b3218a7aae"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'drive' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f8493bda0e08>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#from google.colab import drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"]}]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# batch_size = args.batch_size # to use the batch_size cmd arg -> python file_name.py -batch_size 32\n","batch_size = 64\n","block_size = 128 # text length\n","max_iters = 10000  # number of times that batch run\n","learning_rate = 2e-5\n","eval_iters = 1000\n","n_embd = 384  #embedding size\n","n_head = 4\n","n_layer = 4\n","dropout = 0.2"],"metadata":{"id":"dpCY49ADkUAy","executionInfo":{"status":"ok","timestamp":1712470379318,"user_tz":-480,"elapsed":3,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["pip install datasets"],"metadata":{"id":"WpTBiX7KaSVA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# data preprocess"],"metadata":{"id":"mMi2Gngrry4C"}},{"cell_type":"code","source":["import datasets\n","\n","ds_sft = datasets.load_dataset('TigerResearch/pretrain_en', split='train[:1%]')"],"metadata":{"id":"dI6tLwh9WySW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: just download a part of hugging face dataset\n","\n","!pip install datasets\n","ds_sft = datasets.load_dataset('TigerResearch/pretrain_en', split='train[:10%]')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"QZgBnTCbW0TP","executionInfo":{"status":"error","timestamp":1711467691606,"user_tz":-480,"elapsed":6,"user":{"displayName":"chris peng","userId":"18113039730144201133"}},"outputId":"5565705b-2d1f-4e90-d4e6-785d8f259313"},"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'ds_sft' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-9c6ce9ad727d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prompt: take a look of the first few rows of dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_sft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'ds_sft' is not defined"]}]},{"cell_type":"code","source":["# prompt: give me a batch of this dataset\n","\n","# Create a DataLoader instance to generate batches of data\n","data_loader = torch.utils.data.DataLoader(\n","    ds_sft['train'],\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","# Iterate over the DataLoader to get a batch of data\n","for batch in data_loader:\n","    # Print the batch data\n","    print(batch)\n","    break\n"],"metadata":{"id":"eNU_dgYgar_n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: create a masked language model pretrain function\n","\n","def pretrain_mlm(model, batch, optimizer):\n","    # Set the model to training mode\n","    model.train()\n","\n","    # Get the input tokens and labels\n","    x = batch['input_ids'].to(device)\n","    y = batch['labels'].to(device)\n","\n","    # Generate random masks\n","    mask = torch.rand(x.shape) < 0.15\n","    mask = mask & (x != 0)  # Don't mask special tokens\n","\n","    # Replace masked tokens with a special token\n","    x_masked = x.clone()\n","    x_masked[mask] = 0\n","\n","    # Get the logits from the model\n","    logits = model(x_masked)\n","\n","    # Calculate the loss\n","    loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n","\n","    # Perform backpropagation and update the model parameters\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    return loss.item()\n"],"metadata":{"id":"Q_BYNvd-cQFv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## define a the vocaulary list from target files, and turn chars into indice\n","\n","\n","1.   this is a char level encode\n","\n","\n","\n"],"metadata":{"id":"2NV9V73ul-je"}},{"cell_type":"code","source":["# prompt: copy file in drive/MyDrive/pretrain to content\n","\n","!cp -r /content/drive/MyDrive/pretrainfiles  /content\n"],"metadata":{"id":"yFPg4WcIeNmU","executionInfo":{"status":"ok","timestamp":1712470502528,"user_tz":-480,"elapsed":47432,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["vocab_file=\"/content/pretrainfiles/vocab.txt\""],"metadata":{"id":"XEjfXN5ygCPj","executionInfo":{"status":"ok","timestamp":1712470502528,"user_tz":-480,"elapsed":3,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["chars = \"\"\n","with open(vocab_file, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","        chars = sorted(list(set(text)))\n","\n","vocab_size = len(chars)\n","\n","\n","string_to_int = { ch:i for i,ch in enumerate(chars) }\n","int_to_string = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [string_to_int[c] for c in s]\n","decode = lambda l: ''.join([int_to_string[i] for i in l])\n","\n"],"metadata":{"id":"WI00bOFZlcSx","executionInfo":{"status":"ok","timestamp":1712470525178,"user_tz":-480,"elapsed":547,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["trainfile = \"/content/pretrainfiles/output_opentex.txt\"\n","valfile = \"/content/pretrainfiles/output_opentex.txt\""],"metadata":{"id":"HYw5Zx_-fHnh","executionInfo":{"status":"ok","timestamp":1712470528865,"user_tz":-480,"elapsed":2,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## get batches"],"metadata":{"id":"dcOGkqxdr-Je"}},{"cell_type":"code","source":["# memory map for using small snippets of text from a single file of any size\n","# and it doesn't read in the whole file, good to deal with large files\n","def get_random_chunk(split):\n","    filename = trainfile if split == 'train' else valfile\n","    with open(filename, 'rb') as f:\n","        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n","            # Determine the file size and a random position to start reading\n","            file_size = len(mm)\n","            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n","\n","            # Seek to the random position and read the block of text\n","            mm.seek(start_pos)\n","            block = mm.read(block_size*batch_size-1)\n","\n","            # Decode the block to a string, ignoring any invalid byte sequences\n","            # ?? the files are stored with index, to save space, what if use index without decode and encode\n","            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n","\n","            # Train and test splits\n","            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n","\n","    return data\n","\n","\n","def get_batch(split):\n","    data = get_random_chunk(split)\n","    # for each sample, it is a random block of the chunked data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    # the target is the next token of a seq\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y"],"metadata":{"id":"1wPK66rTlcU7","executionInfo":{"status":"ok","timestamp":1712470533428,"user_tz":-480,"elapsed":1064,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# model\n","\n","1.   framwork\n","2.   decoder\n","3.   feedforward\n","4.   head\n","\n"],"metadata":{"id":"ztv61_Ui0I98"}},{"cell_type":"markdown","source":["## framework"],"metadata":{"id":"EySz1ZN91Zy_"}},{"cell_type":"code","source":["#framwork\n","class GPTLanguageModel(nn.Module):\n","    #class of nn.module means the parameters could be trained\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        #embedding(a,b) create a mapping to dimention b for each a\n","        #这里embedding table是一个把所有vocab映射到embedding的表，所以n_embd是只embedding的长度\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","\n","        #this is nlayers of decoder ,sequential means they are connencted on after another\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        # final layer norm\n","        self.ln_f = nn.LayerNorm(n_embd)\n","        # after norm, convert to dimention of vocab, ready for softmax to get prob for each vocab\n","        # 这里的vocab可以是任意粒度，char,word,subword不一定是单词\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","        #?? directily execute when create the model instance\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        #initialze weights of network with normal distribution\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","        #?? why not intilize bert blocks weights like kqv\n","\n","    def forward(self, index, targets=None):\n","        B, T = index.shape\n","        # input index is index of text\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        # 这是把输入的text index转成embedding\n","        tok_emb = self.token_embedding_table(index) # (B,T,C,   = batch size , text length, 1 ；since index has length 1  )\n","        # 这是输入的每个position生成一个embendding， 所以入参是 arange（textlength）\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","\n","\n","        #each sentence in batch has same position embedding\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        # for generate procedure, do not need target\n","        if targets is None:\n","            loss = None\n","        else:\n","        # train procedure\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, index, max_new_tokens):\n","        # index is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            # since this model is train with seq lenth of block size\n","            index_cond = index[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self.forward(index_cond)\n","            #print(logits.shape())\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # turn (B,T,C)  to  (B,1, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C), here ,c is the distribution of each vocab\n","            # sample from the distribution\n","            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n","        return index\n"],"metadata":{"id":"64NLGgfK0tgH","executionInfo":{"status":"ok","timestamp":1712470539691,"user_tz":-480,"elapsed":1027,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## decoder"],"metadata":{"id":"saRV9_h1_lH_"}},{"cell_type":"code","source":["class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        # heac_size could be defined in other ways\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        #layernorm(x),x可以是个list，如果是一个int，则对最后一个维度\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        y = self.sa(x)\n","        # res first then norm\n","        x = self.ln1(x + y)\n","        y = self.ffwd(x)\n","        x = self.ln2(x + y)\n","        return x"],"metadata":{"id":"JAVdV0Tt_kN0","executionInfo":{"status":"ok","timestamp":1712470542787,"user_tz":-480,"elapsed":2,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## feedforward"],"metadata":{"id":"3qjBmElVAj4A"}},{"cell_type":"code","source":["class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            ##??expand size of e_embd to 4 * n_embd then compress\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"3uOA_CQu_jgg","executionInfo":{"status":"ok","timestamp":1712470546755,"user_tz":-480,"elapsed":3,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["## head"],"metadata":{"id":"25YaalbOC0v_"}},{"cell_type":"code","source":["class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        #after register, doesn't have to create this mask matrix every time\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # input of size (batch, time-step, channels)\n","        # output of size (batch, time-step, head size)\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,hs)\n","        q = self.query(x) # (B,T,hs)\n","        # compute attention scores (\"affinities\")\n","        #?? what is the k.shape item\n","        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,hs)\n","        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n","        return out\n","\n","# [1, 0, 0]\n","# [1, 0.6, 0]\n","# [1, 0.6, 0.4]\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n","        out = self.dropout(self.proj(out))\n","        return out\n",""],"metadata":{"id":"QOfii69vC27l","executionInfo":{"status":"ok","timestamp":1712470546755,"user_tz":-480,"elapsed":2,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## claim"],"metadata":{"id":"Wwvz4p2aP7jd"}},{"cell_type":"code","source":["# prompt: I have model checkpoint in /content/checkpoints/   dir ,   the name of checkpoint file contains the iteration numbers  ,forexample, \"checkpoint_1000.pt\" means it has trained 1000 iters , load the latest checkpoint to model ,and also get the iteration number\n","\n","import os\n","import re\n","\n","def load_latest_checkpoint(model, checkpoint_path):\n","  # Get a list of all checkpoint files in the directory\n","  checkpoint_files = os.listdir(checkpoint_path)\n","\n","  # Find the checkpoint file with the highest iteration number\n","  latest_checkpoint = max(checkpoint_files, key=lambda f: int(re.findall(r'\\d+', f)[0]))\n","\n","  # Load the checkpoint\n","  checkpoint = torch.load(os.path.join(checkpoint_path, latest_checkpoint))\n","\n","  # Load the model state dictionary\n","  model.load_state_dict(checkpoint)\n","\n","  # Get the iteration number\n","  iteration = int(re.findall(r'\\d+', latest_checkpoint)[0])\n","\n","  return model, iteration\n"],"metadata":{"id":"bDbPZMRoOYgv","executionInfo":{"status":"ok","timestamp":1712470916018,"user_tz":-480,"elapsed":825,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model = GPTLanguageModel(vocab_size)\n","#cur_iter=0  # if first trained\n","# print('loading model parameters...')\n","# with open('model-01.pkl', 'rb') as f:\n","#     model = pickle.load(f)\n","# print('loaded successfully!')\n","\n","\n","model,cur_iter= load_latest_checkpoint(model,checkpoint_path)\n","\n","model = model.to(device)\n"],"metadata":{"id":"_bzgBvC-P6mF","executionInfo":{"status":"ok","timestamp":1712470919902,"user_tz":-480,"elapsed":3,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## eval function"],"metadata":{"id":"AS8xDsLlRmS-"}},{"cell_type":"code","source":["#means this process doesnt update gradient\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    #compute loss during tarin every eval_iters for both train and del datasets\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"o9qDV2fPRqnJ","executionInfo":{"status":"ok","timestamp":1712470753094,"user_tz":-480,"elapsed":1101,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["## optimizer\n"],"metadata":{"id":"TzRF6Vn_SDLP"}},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n"],"metadata":{"id":"OTkAgyl9SHFP","executionInfo":{"status":"ok","timestamp":1711616273912,"user_tz":-480,"elapsed":3590,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# train"],"metadata":{"id":"lJvbP2ZnSJpm"}},{"cell_type":"code","source":["\n","for iter in range(max_iters):\n","    if iter % 50 ==0:\n","      print(iter)\n","\n","    if iter % eval_iters  == 0 and iter > 0 :\n","        losses = estimate_loss()\n","        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model.forward(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","print(loss.item())\n"],"metadata":{"id":"11RYNjVLSMTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: I have a model from nn.module, give me a train function, every 2000 step store the current step model to checkpoint, replace the previous one . and the checkpoint file should named by the step counts\n","\n","def train(model, optimizer, max_iters):\n","    for iter in range(max_iters):\n","        iter = iter+cur_iter+1\n","        if iter % 2000 == 0 and iter>0 :\n","            # Save the model checkpoint\n","            ckpt_path =  checkpoint_path + f\"checkpoint_{iter}.pt\"\n","            torch.save(model.state_dict(), ckpt_path)\n","            print(f\"Saved checkpoint at {ckpt_path}\")\n","\n","        if iter % 50 ==0:\n","              print(iter)\n","\n","        if iter % eval_iters  == 0 and iter > 0 :\n","          losses = estimate_loss()\n","          print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n","\n","        # Perform training steps\n","         # sample a batch of data\n","        xb, yb = get_batch('train')\n","\n","        # evaluate the loss\n","        logits, loss = model.forward(xb, yb)\n","        optimizer.zero_grad(set_to_none=True)\n","        loss.backward()\n","        optimizer.step()\n","    print(loss.item())\n"],"metadata":{"id":"LTjqq8njMWKg","executionInfo":{"status":"ok","timestamp":1711617492377,"user_tz":-480,"elapsed":2,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["train(model,optimizer,max_iters)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZiG7ZQgSVM4","outputId":"a343e83f-3ff1-4a48-b2f5-a43d71d5a42d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n","100\n","150\n","200\n","250\n","300\n","350\n","400\n","450\n","500\n","550\n","600\n","650\n","700\n","750\n","800\n","850\n","900\n","950\n","1000\n","step: 1000, train loss: 2.238, val loss: 2.216\n","1050\n","1100\n","1150\n","1200\n","1250\n","1300\n","1350\n","1400\n","1450\n","1500\n","1550\n","1600\n","1650\n","1700\n","1750\n","1800\n","1850\n","1900\n","1950\n","Saved checkpoint at /content/drive/MyDrive/checkpoints/checkpoint_2000.pt\n","2000\n","step: 2000, train loss: 2.140, val loss: 2.140\n","2050\n","2100\n","2150\n","2200\n","2250\n","2300\n","2350\n","2400\n","2450\n","2500\n","2550\n","2600\n","2650\n","2700\n","2750\n","2800\n","2850\n","2900\n","2950\n","3000\n","step: 3000, train loss: 2.040, val loss: 2.045\n","3050\n","3100\n","3150\n","3200\n","3250\n","3300\n","3350\n","3400\n","3450\n","3500\n","3550\n","3600\n","3650\n","3700\n","3750\n","3800\n","3850\n","3900\n","3950\n","Saved checkpoint at /content/drive/MyDrive/checkpoints/checkpoint_4000.pt\n","4000\n","step: 4000, train loss: 1.980, val loss: 1.975\n","4050\n","4100\n","4150\n","4200\n","4250\n","4300\n","4350\n","4400\n","4450\n","4500\n","4550\n","4600\n","4650\n","4700\n","4750\n","4800\n","4850\n","4900\n","4950\n","5000\n","step: 5000, train loss: 1.905, val loss: 1.932\n","5050\n","5100\n","5150\n","5200\n","5250\n","5300\n","5350\n","5400\n","5450\n","5500\n","5550\n","5600\n","5650\n","5700\n","5750\n","5800\n","5850\n","5900\n","5950\n","Saved checkpoint at /content/drive/MyDrive/checkpoints/checkpoint_6000.pt\n","6000\n","step: 6000, train loss: 1.873, val loss: 1.871\n","6050\n","6100\n","6150\n","6200\n","6250\n","6300\n","6350\n","6400\n","6450\n","6500\n","6550\n","6600\n","6650\n","6700\n","6750\n","6800\n","6850\n","6900\n","6950\n","7000\n","step: 7000, train loss: 1.838, val loss: 1.832\n","7050\n","7100\n","7150\n","7200\n","7250\n","7300\n","7350\n","7400\n","7450\n","7500\n","7550\n","7600\n","7650\n","7700\n","7750\n","7800\n","7850\n","7900\n","7950\n","Saved checkpoint at /content/drive/MyDrive/checkpoints/checkpoint_8000.pt\n","8000\n","step: 8000, train loss: 1.810, val loss: 1.808\n","8050\n","8100\n","8150\n","8200\n","8250\n","8300\n","8350\n","8400\n","8450\n","8500\n","8550\n","8600\n","8650\n","8700\n","8750\n","8800\n","8850\n","8900\n","8950\n","9000\n","step: 9000, train loss: 1.771, val loss: 1.781\n","9050\n","9100\n","9150\n","9200\n","9250\n"]}]},{"cell_type":"code","source":["#with open('model-01.pkl', 'wb') as f:\n","#    pickle.dump(model, f)\n","#print('model saved')"],"metadata":{"id":"jOtX0_k1Amaw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: create a checkpoints dir in /content/checkpoints\n","\n","#!mkdir -p /content/drive/MyDrive/checkpoints\n"],"metadata":{"id":"tpiSoEYsV-Ih","executionInfo":{"status":"ok","timestamp":1711617393511,"user_tz":-480,"elapsed":4,"user":{"displayName":"chris peng","userId":"18113039730144201133"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["# infer"],"metadata":{"id":"jHpoftUBD1u0"}},{"cell_type":"code","source":["prompt = 'How are you doing today?'\n","context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n","generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n","print(generated_chars)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WjP81wc3D4W7","executionInfo":{"status":"ok","timestamp":1712470983808,"user_tz":-480,"elapsed":1316,"user":{"displayName":"chris peng","userId":"18113039730144201133"}},"outputId":"ee1842f2-e7bd-4c89-8f46-0b9900d7cfdc"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["How are you doing today?\n","\n","profficeght think is per‘wer gooast on vered-devattebling the thirou\\ firsondite atvew of steplice\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HoTSTPTJGMRP"},"execution_count":null,"outputs":[]}]}